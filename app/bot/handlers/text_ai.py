#!/usr/bin/env python3
# coding: utf-8
"""
Text AI handlers (ChatGPT, Claude, etc.) - Updated with new billing system.
"""
from aiogram import Router, F
from aiogram.types import Message, CallbackQuery
from aiogram.fsm.context import FSMContext

from app.bot.keyboards.inline import ai_models_keyboard, back_to_main_keyboard
from app.bot.states.dialog import AIGenerationStates
from app.database.models.user import User
from app.database.database import async_session_maker
from app.services.billing.billing_service import BillingService
from app.services.subscription.subscription_service import SubscriptionService
from app.core.billing_config import ModelType, get_text_model_billing
from app.core.logger import get_logger
from app.core.exceptions import InsufficientTokensError

logger = get_logger(__name__)

router = Router(name="text_ai")


# Model configurations with new billing
MODEL_CONFIGS = {
    "gpt-4o": {
        "display_name": "GPT-4o",
        "description": "–°–∞–º–∞—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å OpenAI",
        "billing_id": "gpt-4o",
    },
    "gpt-4.1-mini": {
        "display_name": "GPT-4.1 Mini",
        "description": "–ë—ã—Å—Ç—Ä–∞—è –∏ –¥–æ—Å—Ç—É–ø–Ω–∞—è –º–æ–¥–µ–ª—å",
        "billing_id": "gpt-4.1-mini",
    },
    "gpt-5-mini": {
        "display_name": "GPT-5 Mini",
        "description": "–ù–æ–≤–µ–π—à–∞—è –º–∏–Ω–∏-–º–æ–¥–µ–ª—å OpenAI",
        "billing_id": "gpt-5-mini",
    },
    "claude-4": {
        "display_name": "Claude 4",
        "description": "–ú–æ–¥–µ–ª—å –æ—Ç Anthropic",
        "billing_id": "claude-4",
    },
    "gemini-flash-2.0": {
        "display_name": "Gemini Flash 2.0",
        "description": "–ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –æ—Ç Google",
        "billing_id": "gemini-flash-2.0",
    },
    "deepseek-chat": {
        "display_name": "DeepSeek Chat",
        "description": "–û—Ç–ª–∏—á–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞",
        "billing_id": "deepseek-chat",
    },
    "deepseek-r1": {
        "display_name": "DeepSeek R1",
        "description": "Reasoning –º–æ–¥–µ–ª—å DeepSeek",
        "billing_id": "deepseek-r1",
    },
    "o3-mini": {
        "display_name": "O3 Mini",
        "description": "OpenAI Reasoning –º–æ–¥–µ–ª—å",
        "billing_id": "o3-mini",
    },
    "sonar": {
        "display_name": "Sonar (with search)",
        "description": "–ú–æ–¥–µ–ª—å —Å –ø–æ–∏—Å–∫–æ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ",
        "billing_id": "sonar",
    },
    "sonar-pro": {
        "display_name": "Sonar Pro",
        "description": "–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å —Å –ø–æ–∏—Å–∫–æ–º",
        "billing_id": "sonar-pro",
    },
    # Legacy model mappings
    "gpt-4": "gpt-4o",
    "gpt-4-mini": "gpt-4.1-mini",
    "claude": "claude-4",
    "gemini": "gemini-flash-2.0",
    "deepseek": "deepseek-chat",
}


def get_model_config(model_id: str) -> dict:
    """Get model configuration, resolving legacy IDs."""
    config = MODEL_CONFIGS.get(model_id)
    if isinstance(config, str):
        # It's a legacy mapping, resolve it
        return MODEL_CONFIGS[config]
    return config


@router.callback_query(F.data == "chatgpt")
async def start_chatgpt(callback: CallbackQuery, state: FSMContext):
    """Start ChatGPT dialog."""
    model_id = "gpt-4o"
    billing = get_text_model_billing(model_id)

    text = f"""üóØ **ChatGPT (GPT-4o)**

–û—Ç–ø—Ä–∞–≤—å—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º –∏–ª–∏ –≥–æ–ª–æ—Å–æ–≤—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º.

–í—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç–µ –ø—Ä–∏–∫—Ä–µ–ø–∏—Ç—å –¥–æ 10 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.

‚ÑπÔ∏è **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤**
–¢–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã –≤–∞—à–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏.
‚Ä¢ –ë–∞–∑–æ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: {billing.base_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤
‚Ä¢ –ó–∞ –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω AI: {billing.per_gpt_token} –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤"""

    await callback.message.edit_text(
        text,
        reply_markup=back_to_main_keyboard()
    )
    await callback.answer()

    # Set state
    await state.set_state(AIGenerationStates.waiting_for_prompt)
    await state.update_data(ai_model=model_id)


@router.callback_query(F.data == "select_model")
async def select_ai_model(callback: CallbackQuery):
    """Show AI model selection."""

    text = """ü§ñ **–í—ã–±–æ—Ä AI –º–æ–¥–µ–ª–∏**

–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –¥–∏–∞–ª–æ–≥–∞.

‚ÑπÔ∏è **–û —Å—Ç–æ–∏–º–æ—Å—Ç–∏:**
–î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
–°—Ç–æ–∏–º–æ—Å—Ç—å = –±–∞–∑–æ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å + (–≤–∞—à–∏ —Ç–æ–∫–µ–Ω—ã + —Ç–æ–∫–µ–Ω—ã –æ—Ç–≤–µ—Ç–∞) √ó –º–Ω–æ–∂–∏—Ç–µ–ª—å

**–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:**
‚Ä¢ GPT-4o - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å
‚Ä¢ GPT-4.1 Mini - –±—ã—Å—Ç—Ä–∞—è –∏ —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è
‚Ä¢ Claude 4 - –æ—Ç–ª–∏—á–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
‚Ä¢ Gemini Flash 2.0 - —Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è
‚Ä¢ DeepSeek Chat / R1 - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
‚Ä¢ O3 Mini - reasoning –º–æ–¥–µ–ª—å
‚Ä¢ Sonar / Sonar Pro - —Å –ø–æ–∏—Å–∫–æ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""

    await callback.message.edit_text(
        text,
        reply_markup=ai_models_keyboard()
    )
    await callback.answer()


@router.callback_query(F.data.startswith("model:"))
async def choose_model(callback: CallbackQuery, state: FSMContext):
    """Choose AI model and start dialog."""

    model_id = callback.data.split(":")[1]
    config = get_model_config(model_id)

    if not config:
        await callback.answer("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å", show_alert=True)
        return

    billing = get_text_model_billing(config["billing_id"])

    text = f"""‚úÖ **–í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {config['display_name']}**

{config['description']}

–û—Ç–ø—Ä–∞–≤—å—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å.

‚ÑπÔ∏è **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–ø–∏—Å–∞–Ω–∏–µ:**
‚Ä¢ –ë–∞–∑–æ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: {billing.base_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤
‚Ä¢ –ó–∞ —Ç–æ–∫–µ–Ω AI: {billing.per_gpt_token}x
‚Ä¢ –¢–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã –∑–∞–ø—Ä–æ—Å–∞ –∏ –æ—Ç–≤–µ—Ç–∞"""

    await callback.message.edit_text(
        text,
        reply_markup=back_to_main_keyboard()
    )
    await callback.answer()

    # Set state
    await state.set_state(AIGenerationStates.waiting_for_prompt)
    await state.update_data(ai_model=config["billing_id"])


@router.message(AIGenerationStates.waiting_for_prompt)
async def process_ai_request(message: Message, user: User, state: FSMContext):
    """Process AI request with new billing system."""

    # Check message length (max 2000 characters)
    if message.text and len(message.text) > 2000:
        await message.answer(
            "‚ö†Ô∏è –°–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ!\n\n"
            f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 2000 —Å–∏–º–≤–æ–ª–æ–≤\n"
            f"–í–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {len(message.text)} —Å–∏–º–≤–æ–ª–æ–≤\n\n"
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ–∫—Ä–∞—Ç–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
        )
        return

    # Get state data
    data = await state.get_data()
    model_id = data.get("ai_model", "gpt-4o")

    # Get billing configuration
    billing = get_text_model_billing(model_id)
    if not billing:
        await message.answer("‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏")
        await state.clear()
        return

    # Estimate minimum cost (base_tokens + estimated avg usage)
    # Estimate ~500 prompt tokens + ~1000 completion tokens for avg request
    estimated_tokens = billing.calculate_cost(500, 1000)

    # Check if user has enough tokens for estimated cost
    async with async_session_maker() as session:
        sub_service = SubscriptionService(session)

        has_balance = await sub_service.check_token_balance(user.id, estimated_tokens)

        if not has_balance:
            total_tokens = await sub_service.get_available_tokens(user.id)
            await message.answer(
                f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤!\n\n"
                f"–ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞: {estimated_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤\n"
                f"–í–∞—à –±–∞–ª–∞–Ω—Å: {total_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤\n\n"
                f"–ö—É–ø–∏—Ç–µ –ø–æ–¥–ø–∏—Å–∫—É: /start ‚Üí üíé –ü–æ–¥–ø–∏—Å–∫–∞"
            )
            await state.clear()
            return

    # Process with AI using factory
    processing_msg = await message.answer("‚è≥ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å...")

    try:
        from app.services.ai.ai_factory import AIServiceFactory

        # Factory will auto-detect if API keys are available or use mock
        response = await AIServiceFactory.generate_text(
            model=model_id,
            prompt=message.text
        )

        if not response.success:
            await processing_msg.edit_text(
                f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:\n{response.error}"
            )
            logger.error(
                "ai_request_failed",
                user_id=user.id,
                model=model_id,
                error=response.error
            )
            await state.clear()
            return

        # Calculate actual cost based on real usage
        async with async_session_maker() as session:
            billing_service = BillingService(session)

            # If we have token information from API, use it
            if response.prompt_tokens > 0 and response.completion_tokens > 0:
                prompt_tokens = response.prompt_tokens
                completion_tokens = response.completion_tokens
            else:
                # Fallback: estimate based on text length
                # Rough estimate: ~4 characters per token
                prompt_tokens = max(len(message.text) // 4, 100)
                completion_tokens = max(len(response.content) // 4, 100)

            try:
                actual_cost, ai_request = await billing_service.calculate_and_charge_text(
                    user_id=user.id,
                    model_id=model_id,
                    prompt_tokens=prompt_tokens,
                    completion_tokens=completion_tokens,
                    prompt=message.text,
                )

                # Mark request as completed
                await billing_service.mark_request_completed(ai_request.id)

                # Format response
                content = response.content
                if response.metadata.get("mock"):
                    content += "\n\n‚ö†Ô∏è _–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º (API –∫–ª—é—á–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã)_"

                # Add cost information
                content += f"\n\nüí∞ –°–ø–∏—Å–∞–Ω–æ: {actual_cost:,} —Ç–æ–∫–µ–Ω–æ–≤"

                await processing_msg.edit_text(content)

                logger.info(
                    "ai_request_completed",
                    user_id=user.id,
                    model=model_id,
                    prompt_tokens=prompt_tokens,
                    completion_tokens=completion_tokens,
                    cost=actual_cost,
                    is_mock=response.metadata.get("mock", False)
                )

            except InsufficientTokensError as e:
                await processing_msg.edit_text(
                    f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–ø–ª–∞—Ç—ã –∑–∞–ø—Ä–æ—Å–∞!\n\n"
                    f"–¢—Ä–µ–±—É–µ—Ç—Å—è: {actual_cost:,} —Ç–æ–∫–µ–Ω–æ–≤\n"
                    f"–î–æ—Å—Ç—É–ø–Ω–æ: {e.details.get('available', 0):,} —Ç–æ–∫–µ–Ω–æ–≤\n\n"
                    f"–ö—É–ø–∏—Ç–µ –ø–æ–¥–ø–∏—Å–∫—É: /start ‚Üí üíé –ü–æ–¥–ø–∏—Å–∫–∞"
                )
                logger.warning(
                    "ai_request_insufficient_tokens",
                    user_id=user.id,
                    model=model_id,
                    required=actual_cost
                )

    except Exception as e:
        await processing_msg.edit_text(
            f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}"
        )
        logger.error("ai_request_exception", user_id=user.id, model=model_id, error=str(e))

    await state.clear()
